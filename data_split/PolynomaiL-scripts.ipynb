{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "835bce27-fbd1-417d-842c-3297d7b8969b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 203334\n",
      "Model performance on training data: 1669.4345474243164\n",
      "Model performance on validation data 1791.1510467529297\n",
      "Model performance on blind randomized testing data: 305.8645963668823\n",
      "Corrected model performance on blind randomized testing data: 459.8729610443115\n",
      "Time 6.201056718826294\n",
      "training 406666\n",
      "Model performance on training data: 1370.8828926086426\n",
      "Model performance on validation data 1528.0974388122559\n",
      "Model performance on blind randomized testing data: 254.7797679901123\n",
      "Corrected model performance on blind randomized testing data: 386.35880947113037\n",
      "Time 11.288595199584961\n",
      "training 610000\n",
      "Model performance on training data: 1364.307689666748\n",
      "Model performance on validation data 1169.0274238586426\n",
      "Model performance on blind randomized testing data: 236.87222003936768\n",
      "Corrected model performance on blind randomized testing data: 356.25202655792236\n",
      "Time 17.344545602798462\n",
      "training 813334\n",
      "Model performance on training data: 1262.6300811767578\n",
      "Model performance on validation data 1276.5830039978027\n",
      "Model performance on blind randomized testing data: 226.64084434509277\n",
      "Corrected model performance on blind randomized testing data: 338.04852962493896\n",
      "Time 22.78384757041931\n",
      "training 1016666\n",
      "Model performance on training data: 1192.8065299987793\n",
      "Model performance on validation data 1271.7238426208496\n",
      "Model performance on blind randomized testing data: 218.34938526153564\n",
      "Corrected model performance on blind randomized testing data: 324.4229555130005\n",
      "Time 28.023701429367065\n",
      "training 1220000\n",
      "Model performance on training data: 1132.9570770263672\n",
      "Model performance on validation data 1131.066608428955\n",
      "Model performance on blind randomized testing data: 208.92634391784668\n",
      "Corrected model performance on blind randomized testing data: 311.06014251708984\n",
      "Time 34.33258247375488\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.linear_model import Ridge as cpu_Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from tqdm.auto import tqdm, trange\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler\n",
    "from skorch.dataset import Dataset\n",
    "from skorch.helper import predefined_split\n",
    "import joblib\n",
    "from helpful_functions import InputLogTransformer, OutputLogTransformer, build_neural_network, make_poly_datasets\n",
    "import time\n",
    "\n",
    "spectrum = False\n",
    "noise = 10\n",
    "if spectrum:\n",
    "    num_outputs = 25\n",
    "    identifier = 'spectrum'\n",
    "    output_list = ['Bin ' + str(i) for i in range(25)] # training outputs\n",
    "else:\n",
    "    num_outputs = 3\n",
    "    identifier = 'threeEns'\n",
    "    output_list = ['Max Proton Energy', 'Total Proton Energy', 'Avg Proton Energy']\n",
    "dfA = pd.read_hdf(f'datasets/fuchs_v5_0_seed-2_train_1274091_noise_{noise}_{identifier}_campaign2.h5', key = 'df')\n",
    "dfB = pd.read_hdf(f'datasets/fuchs_v5_0_seed-2_train_campaign1_pct_100_noise_{noise}.h5', key='df')\n",
    "#df = pd.concat([dfA, dfB], ignore_index=True).fillna(0)\n",
    "df = pd.read_hdf(f'datasets/fuchs_v5_0_seed-2_train_1525000_noise_{noise}_threeEns_.h5', key='df').fillna(0)\n",
    "df.loc[:, output_list] = df.loc[:, output_list].clip(1e-2, None)\n",
    "test_df = pd.read_hdf(f'datasets/fuchs_v5_0_seed-2_test_1000000_noise_0_{identifier}_.h5', key = 'df').fillna(0)\n",
    "test_df.loc[:, output_list] = test_df.loc[:, output_list].clip(1e-2, None)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "datype = np.float32\n",
    "num_inputs = 4\n",
    "num_outputs = 3\n",
    "batch_size = 2**14\n",
    "patience=5\n",
    "\n",
    "input_list = ['Intensity', 'Target Thickness', 'Focal Distance', 'Contrast'] # independent variables\n",
    "time_list = []\n",
    "mape_list = []\n",
    "point_list = []\n",
    "uncorrected_list = []\n",
    "# testing_list = ['Max Exact Energy', 'Total Exact Energy', 'Avg Exact Energy'] # testing set outputs, deprecated\n",
    "\n",
    "#shuffle dataset to allow for random sampling\n",
    "df = df.sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "X = np.array(df[input_list],dtype=datype)\n",
    "y = np.array(df[output_list],dtype=datype)\n",
    "#X_train0, y_train0, X_val0, y_val0, input_transformer, output_transformer = make_datasets(X, y, random_state=42)\n",
    "data_fractions = np.linspace(0, 1, 7)[1:]\n",
    "num_pts_tot = X.shape[0]\n",
    "\n",
    "X_test = np.array(test_df[input_list],dtype=datype)\n",
    "y_test = np.array(test_df[output_list],dtype=datype)\n",
    "\n",
    "# Polynomial determined by hyperparameter optimization\n",
    "degree = 7\n",
    "alpha = 1e-3\n",
    "\n",
    "for frac in data_fractions:\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    t0 = time.time()\n",
    "    num_pts = int(num_pts_tot * frac + 0.5)\n",
    "    num_train_pts = int(0.8 * num_pts + 0.5)\n",
    "    print(\"training\", num_train_pts)\n",
    "    X_train, y_train, X_val, y_val, input_transformer, output_transformer = make_poly_datasets(X[:num_pts, :], y[:num_pts, :], random_state=42)\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    ridge = cpu_Ridge(alpha=alpha)\n",
    "    model = make_pipeline(poly, ridge)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = output_transformer.inverse_transform(model.predict(X_val))\n",
    "    y_train_pred = output_transformer.inverse_transform(model.predict(X_train))\n",
    "    y_train_true = output_transformer.inverse_transform(y_train)\n",
    "    correction_factors = []\n",
    "    for i in range(num_outputs):\n",
    "        α = np.mean(y_train_true[:, i]/y_train_pred[:, i])\n",
    "        correction_factors.append(α)\n",
    "    y_train_pred_corrected = y_train_pred.copy()\n",
    "    for i in range(num_outputs):\n",
    "        y_train_pred_corrected[:, i] *= correction_factors[i]\n",
    "    X_train_true = input_transformer.inverse_transform(X_train)\n",
    "    y_val_true = output_transformer.inverse_transform(y_val)\n",
    "    y_train_pred = output_transformer.inverse_transform(model.predict(X_train))\n",
    "    X_test_scaled = input_transformer.transform(X_test)\n",
    "    y_test_pred = output_transformer.inverse_transform(model.predict(X_test_scaled))\n",
    "    y_test_pred_corrected = y_test_pred.copy()\n",
    "    for i in range(num_outputs):\n",
    "        y_test_pred_corrected[:, i] *= correction_factors[i]\n",
    "    print(\"Model performance on training data:\", mean_absolute_percentage_error(y_train_true, y_train_pred)*100)\n",
    "    print(\"Model performance on validation data\", mean_absolute_percentage_error(y_val_true, y_pred)*100)\n",
    "    mape_uncorrected = mean_absolute_percentage_error(y_test, y_test_pred)*100\n",
    "    print(\"Model performance on blind randomized testing data:\", mape_uncorrected)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_test_pred_corrected)*100\n",
    "    print(\"Corrected model performance on blind randomized testing data:\", mape)\n",
    "    t1 = time.time()\n",
    "    del_t = t1 - t0\n",
    "    print(\"Time\", del_t)\n",
    "    time_list.append(del_t)\n",
    "    mape_list.append(mape)\n",
    "    point_list.append(num_train_pts)\n",
    "    uncorrected_list.append(mape_uncorrected)\n",
    "\n",
    "df = pd.DataFrame({'points': point_list, 'mape': uncorrected_list, 'corrected': mape_list, 'time': time_list})\n",
    "df.to_csv('results/Poly_data_split.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4abe251b-a135-4a2d-959d-83dbd5dc45f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 0% noise\n",
      "Model performance on training data: 4.24430043922432e+16\n",
      "Model performance on validation data 4.35588771610624e+16\n",
      "Model performance on blind randomized testing data: 208.89930725097656\n",
      "Corrected model performance on blind randomized testing data: 309.9773406982422\n",
      "Time 35.72807478904724\n",
      "Training on 5% noise\n",
      "Model performance on training data: 4.24203081744384e+16\n",
      "Model performance on validation data 4.35017339633664e+16\n",
      "Model performance on blind randomized testing data: 208.61282348632812\n",
      "Corrected model performance on blind randomized testing data: 309.9245071411133\n",
      "Time 35.65686058998108\n",
      "Training on 10% noise\n",
      "Model performance on training data: 2.17431159078912e+16\n",
      "Model performance on validation data 2.08823876845568e+16\n",
      "Model performance on blind randomized testing data: 208.92634391784668\n",
      "Corrected model performance on blind randomized testing data: 311.06014251708984\n",
      "Time 35.55851674079895\n",
      "Training on 15% noise\n",
      "Model performance on training data: 4.21266733400064e+16\n",
      "Model performance on validation data 4.30461620846592e+16\n",
      "Model performance on blind randomized testing data: 206.73935413360596\n",
      "Corrected model performance on blind randomized testing data: 310.0161552429199\n",
      "Time 35.344338178634644\n",
      "Training on 20% noise\n",
      "Model performance on training data: 4.1783455121408e+16\n",
      "Model performance on validation data 4.24803370532864e+16\n",
      "Model performance on blind randomized testing data: 205.32336235046387\n",
      "Corrected model performance on blind randomized testing data: 310.22348403930664\n",
      "Time 35.643025159835815\n",
      "Training on 25% noise\n",
      "Model performance on training data: 4.13379160178688e+16\n",
      "Model performance on validation data 4.20942597586944e+16\n",
      "Model performance on blind randomized testing data: 203.59117984771729\n",
      "Corrected model performance on blind randomized testing data: 310.4538917541504\n",
      "Time 35.39087986946106\n",
      "Training on 30% noise\n",
      "Model performance on training data: 4.0926491705344e+16\n",
      "Model performance on validation data 4.20735801622528e+16\n",
      "Model performance on blind randomized testing data: 201.66609287261963\n",
      "Corrected model performance on blind randomized testing data: 310.72094440460205\n",
      "Time 35.39430212974548\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.linear_model import Ridge as cpu_Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from tqdm.auto import tqdm, trange\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler\n",
    "from skorch.dataset import Dataset\n",
    "from skorch.helper import predefined_split\n",
    "import joblib\n",
    "from helpful_functions import make_poly_datasets\n",
    "import time\n",
    "\n",
    "spectrum = False\n",
    "noise = 10\n",
    "if spectrum:\n",
    "    num_outputs = 25\n",
    "    identifier = 'spectrum'\n",
    "    output_list = ['Bin ' + str(i) for i in range(25)] # training outputs\n",
    "else:\n",
    "    num_outputs = 3\n",
    "    identifier = 'threeEns'\n",
    "    output_list = ['Max Proton Energy', 'Total Proton Energy', 'Avg Proton Energy']\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "datype = np.float32\n",
    "num_inputs = 4\n",
    "num_outputs = 3\n",
    "\n",
    "input_list = ['Intensity', 'Target Thickness', 'Focal Distance', 'Contrast'] # independent variables\n",
    "time_list = []\n",
    "mape_list = []\n",
    "noise_list = []\n",
    "uncorrected_list = []\n",
    "\n",
    "noise_range = np.arange(0, 31, 5)\n",
    "\n",
    "\n",
    "test_df = pd.read_hdf(f'datasets/fuchs_v5_0_seed-2_test_1000000_noise_0_{identifier}_.h5', key = 'df').fillna(0)\n",
    "test_df.loc[:, output_list] = test_df.loc[:, output_list].clip(1e-2, None)\n",
    "\n",
    "# Polynomial hyperparameters determined by optimization\n",
    "degree = 7\n",
    "alpha = 1e-3\n",
    "\n",
    "for noise in noise_range:\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    t0 = time.time()\n",
    "\n",
    "    df = pd.read_hdf(f'datasets/fuchs_v5_0_seed-2_train_1525000_noise_{noise}_threeEns_.h5', key='df').fillna(0)\n",
    "    df.loc[:, output_list] = df.loc[:, output_list].clip(1e-2, None)\n",
    "\n",
    "\n",
    "    # testing_list = ['Max Exact Energy', 'Total Exact Energy', 'Avg Exact Energy'] # testing set outputs, deprecated\n",
    "\n",
    "    #shuffle dataset to allow for random sampling\n",
    "    df = df.sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "    X = np.array(df[input_list],dtype=datype)\n",
    "    y = np.array(df[output_list],dtype=datype)\n",
    "    #X_train0, y_train0, X_val0, y_val0, input_transformer, output_transformer = make_datasets(X, y, random_state=42)\n",
    "\n",
    "    X_test = np.array(test_df[input_list],dtype=datype)\n",
    "    y_test = np.array(test_df[output_list],dtype=datype)\n",
    "\n",
    "    print(f\"Training on {noise}% noise\")\n",
    "    X_train, y_train, X_val, y_val, input_transformer, output_transformer = make_poly_datasets(X, y, random_state=42)\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    ridge = cpu_Ridge(alpha=alpha)\n",
    "    model = make_pipeline(poly, ridge)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = output_transformer.inverse_transform(model.predict(X_val))\n",
    "    y_train_pred = output_transformer.inverse_transform(model.predict(X_train))\n",
    "    y_train_true = output_transformer.inverse_transform(y_train)\n",
    "    correction_factors = []\n",
    "    for i in range(num_outputs):\n",
    "        α = np.mean(y_train_true[:, i]/y_train_pred[:, i])\n",
    "        correction_factors.append(α)\n",
    "    y_train_pred_corrected = y_train_pred.copy()\n",
    "    for i in range(num_outputs):\n",
    "        y_train_pred_corrected[:, i] *= correction_factors[i]\n",
    "    X_train_true = input_transformer.inverse_transform(X_train)\n",
    "    y_val_true = output_transformer.inverse_transform(y_val)\n",
    "    y_train_pred = output_transformer.inverse_transform(model.predict(X_train))\n",
    "    X_test_scaled = input_transformer.transform(X_test)\n",
    "    y_test_pred = output_transformer.inverse_transform(model.predict(X_test_scaled))\n",
    "    y_test_pred_corrected = y_test_pred.copy()\n",
    "    for i in range(num_outputs):\n",
    "        y_test_pred_corrected[:, i] *= correction_factors[i]\n",
    "    print(\"Model performance on training data:\", mean_absolute_percentage_error(y_train, y_train_pred)*100)\n",
    "    print(\"Model performance on validation data\", mean_absolute_percentage_error(y_val, y_pred)*100)\n",
    "    mape_uncorrected = mean_absolute_percentage_error(y_test, y_test_pred)*100\n",
    "    print(\"Model performance on blind randomized testing data:\", mape_uncorrected)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_test_pred_corrected)*100\n",
    "    print(\"Corrected model performance on blind randomized testing data:\", mape)\n",
    "    t1 = time.time()\n",
    "    del_t = t1 - t0\n",
    "    print(\"Time\", del_t)\n",
    "    time_list.append(del_t)\n",
    "    mape_list.append(mape)\n",
    "    noise_list.append(noise)\n",
    "    uncorrected_list.append(mape_uncorrected)\n",
    "\n",
    "    df = pd.DataFrame({'noise level': noise_list, 'mape': uncorrected_list, 'corrected': mape_list, 'time': time_list})\n",
    "    df.to_csv('results/Poly_noise_split.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d395d190-2e43-47bf-a79d-8bbce6cef67c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldiaml [ldiaml]",
   "language": "python",
   "name": "conda_ldiaml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
